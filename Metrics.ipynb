{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different metrics that we can use in machine learning to evaluate the performance of a machine learning algorithm.  This notebook provides a summary of some of the most common ones encountered in typical machine learning applications, with all metrics coded from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed to anything else, here are a few statistical helper functions that we can either use in conjunction with a particular metric or use by itself: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mean \n",
    "\n",
    "The mean is simply the total sum of values in a list divided by the length of the list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(list1):\n",
    "    return sum(list1) / len(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean([1, 3, 5, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Variance \n",
    "\n",
    "The variance is simply the expectation of the squared deviation of a random variable from its mean.  Informally, this means how spread out a set of some numbers are from their average value.  Sample variance is divided by `n-1` instead of `n` in order to remain an unbiased estimator of the population variance.  The sample variance is defined as:\n",
    "\n",
    "$ S^{2} = \\frac{\\Sigma (x_{i} - \\bar{x})^{2}}{n-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance(list1):\n",
    "    return sum((xi - mean(list1)) ** 2 for xi in list1) / len(list1) - 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance([1, 3, 5, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Standard Deviation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation is the square root of the variance and is also a measure of the spread of a set of some numbers from their average value, but unlike the variance, the standard deviation is expressed in the same units as the mean.  The sample standard deviation is defined as:\n",
    "\n",
    "$\\sigma = \\sqrt{\\frac{\\Sigma (x_{i} - \\bar{x})^{2}}{n-1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_deviation(list1):\n",
    "    return (sum((xi - mean(list1)) ** 2 for xi in list1) / len(list1) - 1) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_deviation([1, 3, 5, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common metrics for classification include: \n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n",
    "- Area under the ROC curve\n",
    "- Log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceding to the listed metrics, it is convenient if we define four helper functions first.  The results of a binary classification prediction have four possible outcomes: true positive (TP), true negative (TP), false positive (FP), or false negative (FN).  We can then use these to define the metrics we will use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive(y_true, y_pred): \n",
    "    tp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 1: \n",
    "            tp += 1\n",
    "    return tp\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    tn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 0:\n",
    "            tn += 1\n",
    "    return tn\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    fp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 1:\n",
    "            fp += 1\n",
    "    return fp\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    fn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 0:\n",
    "            fn += 1\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Accuracy \n",
    "\n",
    "Accuracy is defined as the number of true positives divided by the total number of true positives and true negatives:\n",
    "\n",
    "Accuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    tn = true_negative(y_true, y_pred)\n",
    "    accuracy_score = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [0, 1, 1, 1, 0, 0, 0, 1]\n",
    "l2 = [0, 1, 0, 1, 0, 1, 0, 0]\n",
    "accuracy(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Precision \n",
    "\n",
    "Precision is a metric often used in cases where accuracy does not provide useful information about the misclassifications, particularly when imbalanced classes are present.  It is defined as the number of true positives divided by the total number of true and false positives: \n",
    "\n",
    "Precision = $\\frac{TP}{TP + FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    precision_score = tp / (tp + fp)\n",
    "    return precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [0, 1, 1, 1, 0, 0, 0, 1]\n",
    "l2 = [0, 1, 0, 1, 0, 1, 0, 0]\n",
    "precision(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Recall \n",
    "\n",
    "Recall is defined as the number of true positives divided by the total number of true positives and false negatives:\n",
    "\n",
    "Recall = $\\frac{TP}{TP + FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    recall_score = tp / (tp + fn)\n",
    "    return recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [0, 1, 1, 1, 0, 0, 0, 1]\n",
    "l2 = [0, 1, 0, 1, 0, 1, 0, 0]\n",
    "recall(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. F1 Score\n",
    "\n",
    "The F1 score is a metric that combines precision and recall together.  It is mathematically defined as the harmonic mean of precision and recall: \n",
    "\n",
    "F1 = $\\frac{2PR}{P + R} = \\frac{2TP}{2TP + FP + FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    f1_score = 2 * p * r / (p + r)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [0, 1, 1, 1, 0, 0, 0, 1]\n",
    "l2 = [0, 1, 0, 1, 0, 1, 0, 0]\n",
    "f1(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Area under ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing further, let's stop for a second and define a few other functions.  Another common name one sees when looking at classification metrics is the **true positive rate**, or TPR.  This is synonymous and identical to recall or sensitivity, but different names for these are used in different contexts.  This is again defined as:\n",
    "\n",
    "TPR = $\\frac{TP}{TP + FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr(y_true, y_pred):\n",
    "    return recall(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **false positive rate**, or FPR, is defined as: \n",
    "\n",
    "FPR = $\\frac{FP}{TN + FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpr(y_true, y_pred):\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    tn = true_negative(y_true, y_pred)\n",
    "    return fp / (tn + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how these are useful, consider a binary classifier where we have 15 samples and binary target values along with the predicted probabilities of when a sample is positive. Normally, we choose 0.5 as a threshold for binary classification.  But what's to stop us from choosing any threshold between 0 and 1? We can still evaluate the precision, recall, F1, etc. regardless of whichever threshold we choose.  Let's try using only TPR and FPR: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tpr_values = []\n",
    "fpr_values = []\n",
    "\n",
    "# Actual targets\n",
    "y_true = [0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1]\n",
    "\n",
    "# Predicted probabilities of a sample being 1\n",
    "y_pred = [0.1, 0.3, 0.2, 0.6, 0.4, 0.8, 0.9, 0.5, 0.3, 0.95, 0.3, 0.8, 0.85, 0.15, 0.99]\n",
    "\n",
    "# Designated thresholds\n",
    "thresholds = [0, 0.1, 0.2, 0.3, 0.5, 0.5, 0.5, 0.7, 0.8, 0.85, 0.9, 0.99, 1.0]\n",
    "\n",
    "for thresh in thresholds:\n",
    "    temp_pred = [1 if x >= thresh else 0 for x in y_pred]\n",
    "    temp_tpr = tpr(y_true, temp_pred)\n",
    "    temp_fpr = fpr(y_true, temp_pred)\n",
    "    tpr_values.append(temp_tpr)\n",
    "    fpr_values.append(temp_fpr)\n",
    "    \n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.fill_between(fpr_values, tpr_values, alpha=0.4)\n",
    "plt.plot(fpr_values, tpr_values, lw=3)\n",
    "plt.xlim(0, 1.0)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xlabel('FPR', fontsize=15)\n",
    "plt.ylabel('TPR', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value we care about here if the area under the curve of the plot shown above.  We could write it from scratch, but it is much more convenient to just use the scikit-learn implementation of it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9074074074074074"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_true = [0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1]\n",
    "y_pred = [0.1, 0.3, 0.2, 0.6, 0.4, 0.8, 0.9, 0.5, 0.3, 0.95, 0.3, 0.8, 0.85, 0.15, 0.99]\n",
    "roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the score mean? If we get a score of 0.90 as we did above, this basically means that if we select a random sample from our dataset that is a positive sample and a random sample that is negative, the positive sample will rank higher than the negative sample with probability 0.90.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Log-loss\n",
    "\n",
    "For the case of binary classification, the log-loss is defined as: \n",
    "\n",
    "*Log-loss*  = *-1.0 * (target * log(prediction) + (1 - target) * log(1 - prediction)*\n",
    "\n",
    "The log-loss generally penalizes much higher than many other classification metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    loss = []\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        yp = np.clip(yp, epsilon, 1 - epsilon)\n",
    "        temp_loss = -1.0 * (yt * np.log(yp) + (1 - yt) * np.log(1 - yp))\n",
    "        loss.append(temp_loss)\n",
    "    return sum(loss) / len(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4397925137338295"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1]\n",
    "y_pred = [0.1, 0.3, 0.2, 0.6, 0.4, 0.8, 0.9, 0.5, 0.3, 0.95, 0.3, 0.8, 0.85, 0.15, 0.99]\n",
    "log_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common form of metrics in regression is **error**.  Error is simply something analagous to the following: \n",
    "\n",
    "*Error* = *True Value - Predicted Value*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common metrics for regression include: \n",
    "- Mean absolute error (MAE)\n",
    "- Mean squared error (MSE)\n",
    "- Root mean squared error (RMSE)\n",
    "- Mean squared logarithmic error (MSLE)\n",
    "- Root mean squared logarithmic error (RMSLE)\n",
    "- Mean percentage error (MPE)\n",
    "- Mean absolute percentage error (MAPE)\n",
    "- R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mean absolute error (MAE)\n",
    "\n",
    "This is defined simply as the mean value of all absolute errors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true, y_pred):\n",
    "    error = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        error += np.abs(yt - yp)\n",
    "    return error / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.75"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [5, 8, 4, 3]\n",
    "l2 = [6, 5, 4, 6]\n",
    "mean_absolute_error(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mean squared error (MSE)\n",
    "\n",
    "Similarly to MAE, the squared error is defined as:\n",
    "\n",
    "*Squared Error = (True Value - Predicted Value)*^2\n",
    "\n",
    "and the mean squared error is simply the mean value of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    error = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        error += (yt - yp) ** 2\n",
    "    return error / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.75"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [5, 8, 4, 3]\n",
    "l2 = [6, 5, 4, 6]\n",
    "mean_squared_error(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Root mean squared error (RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The root mean squared error is just the square root of the MSE.  MSE and RMSE are the most popular metrics typically used to evaluate regression models.  \n",
    "\n",
    "*RMSE = SQRT(MSE)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    error = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        error += (yt - yp) ** 2\n",
    "    return (error / len(y_true)) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.179449471770337"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [5, 8, 4, 3]\n",
    "l2 = [6, 5, 4, 6]\n",
    "rmse(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Mean squared logarithmic error (MSLE)\n",
    "\n",
    "MSLE is a variation of MSE and can be interpreted as a measure of the ratio between the true and predicted values. The introduction of the logarithm makes MSLE only care about the relative difference between the true and the predicted value, or in other words, it only cares about the percentual difference between them.\n",
    "This means that MSLE will treat small differences between small true and predicted values approximately the same as big differences between large true and predicted values. The squared logarithmic error is:\n",
    "\n",
    "$ \\Sigma((log(1 + y_{i}) - log(1 + \\hat{y}))^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msle(y_true, y_pred):\n",
    "    error = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        error += (np.log(1 + yt) - np.log(1 + yp)) ** 2\n",
    "    return error / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12533355402273888"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [5, 8, 4, 3]\n",
    "l2 = [6, 5, 4, 6]\n",
    "msle(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Root mean squared logarithmic error (RMSLE)\n",
    "\n",
    "This is simply the quare root of the MSLE:\n",
    "\n",
    "*RMSLE = SQRT(MSLE)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    error = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        error += (np.log(1 + yt) - np.log(1 + yp)) ** 2\n",
    "    return (error / len(y_true)) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3540247929492211"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [5, 8, 4, 3]\n",
    "l2 = [6, 5, 4, 6]\n",
    "rmsle(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Mean percentage error (MPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage error is the difference between the true and predicted value, divided by the true value, then multiplied by 100 to convert into a percentage. \n",
    "\n",
    "*Percentage Error = ((True Value - Predicted Value) / True Value) * 100*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_percentage_error(y_true, y_pred):\n",
    "    error = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        error += (yt - yp) / yt\n",
    "    return error / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.20625"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [5, 8, 4, 3]\n",
    "l2 = [6, 5, 4, 6]\n",
    "mean_percentage_error(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 7.  Mean absolute percentage error (MAPE)\n",
    " \n",
    " This is somewhat more common than MPE and is just an absolute value version of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    error = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        error += np.abs(yt - yp) / yt\n",
    "    return error / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39375"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [5, 8, 4, 3]\n",
    "l2 = [6, 5, 4, 6]\n",
    "mape(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 8.  R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R^2 is one of the most common metrics in regression and basically is a measure of how well our model fits the data.  A value closer to 1 implies a better fit, with 1 being the maximum value. R^2 is defined mathematically as 1 - RSS / TSS:\n",
    "\n",
    "$R^{2} = 1 - \\frac{\\Sigma (y_{t}-y_{p})^{2}}{\\Sigma (y_{t}-\\bar{y})^{2}}$\n",
    "\n",
    "So when the RSS is minimized for a given TSS, we get a value closer to one.  It should be noted that this is the **unadjusted** form of R^2, which does not penalize for a large number of useless features in a model.  As we add features to a regression model, R^2 increases regardless if our model is truly capturing the structure in the data better or not, since there are more ways to fit it with increased features.  For this reason, adjusted R^2 is often used, but is not shown here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y_true, y_pred):\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        numerator += (yt - yp) ** 2\n",
    "        denominator += (yt - mean(y_true)) ** 2\n",
    "    ratio = numerator / denominator\n",
    "    return 1 - ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9486081370449679"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "r_squared(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
